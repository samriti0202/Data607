---
title: "TextMinning_Modelling_Project4"
author: "Samriti Malhotra & Vishal Arora"
date: "April 13, 2019"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    code_folding: hide
    highlight: tango
    number_sections: yes
    smooth_scroll: yes
    theme: united
    toc: yes
    toc_collapsed: yes
    toc_depth: 3
    toc_float: yes
---

#Introduction
As part of Project 4 we are trying to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, we used spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam. Example corpus is taken from path:  https://spamassassin.apache.org/publiccorpus/

```{r install-library-list, eval=TRUE, include=FALSE, echo=TRUE}
# load packages
library(RCurl)
library(XML)
library(stringr)
library(tm)
library(SnowballC)
library(rpart)
library(e1071)
library(tidyr)

library("caTools")
library(rpart)
library("rpart.plot")
library(ROCR)
library("randomForest")
library(kableExtra)
library(mlbench)
```
#Problem Statment
Download the spam/ham file and write them into csv file, which has two columns text and spam indicator.

#Solution              

##Step1:-              
In our soltuion is to get a common csv file constructed from the spam and hard ham files ,We wrote a function which will take 3 parameteres , filename, where to construct the csv file and column to be added to final csv file ie. spam=TRUE for email which are span and spam=FALSE for emails which are not spam.                         
```{r eval=FALSE}
txt2csv <- function(myfiles, mycsvfilename , indicator  ){
  
starting_dir <- getwd()
# create a list of dataframes containing the text
mytxts <- lapply(myfiles, readLines)
# combine the rows of each dataframe to make one
# long character vector where each item in the vector
# is a single text file
mytxts1lines <- unlist(mytxts)
mytxts1lines <- paste(mytxts1lines , collapse = ",")
mytxts1lines <- str_replace_all(mytxts1lines, ",", " ")
# make a dataframe with the file names and texts
mytxtsdf <- data.frame(filename = basename(myfiles),fulltext = mytxts1lines) 
#dropping the first column and  binding column wise using cbinf.
mytxtsdf <- cbind(dplyr::select(mytxtsdf,-1),indicator)
  
  # Now write them all into a single CSV file, one txt file per row
  setwd(mydir) # make sure the CSV goes into the dir where the txt files are

    # write the CSV file...
    write.table(mytxtsdf, file = paste0(mycsvfilename, ".csv"),sep=",", row.names = FALSE, col.names = FALSE, quote = FALSE,append = TRUE)
  
  # return original working directory
  setwd(starting_dir)
}
setwd("C:/CUNY_AUG27/Data607/Chapter10/corpus")
mydir <- "C:/CUNY_AUG27/Data607/Chapter10/corpus/spam_2/"
myfiles <- list.files(mydir, full.names = TRUE)
getwd()
for(i in 1:length(myfiles)){
  print(myfiles[i])
  txt2csv(myfiles[i], "C:/CUNY_AUG27/Data607/Chapter10/datacsvfile" , TRUE)
}
mydir <- "C:/CUNY_AUG27/Data607/Chapter10/corpus/hard_ham/"
myfiles <- list.files(mydir, full.names = TRUE)
for(i in 1:length(myfiles)){
  #print(myfiles[i])
  txt2csv(myfiles[i], "C:/CUNY_AUG27/Data607/Chapter10/datacsvfile" , FALSE)
}


```
                    
##Steps2:-                     
This section  , has been commented if you want to run in your local set echo=TRUE chunk parameters and run to construct the csv file , and also if by any chance you want to build again then please delete the old one.                            
```{r warning=FALSE ,echo=FALSE  }
emailDF <- read.csv("datacsvfile.csv", stringsAsFactors = FALSE)
colnames(emailDF) <- c("text","spamIndic") 
str(emailDF)
#printing the value of first column
kable(head(strwrap(emailDF$text[1]))) %>%
  kable_styling(bootstrap_options = c("striped","condensed","responsive"),full_width   = F,position = "left",font_size = 12) %>%
  row_spec(0, background ="gray")

# checking the first row 2nd column value i.e. responsive variable.

kable(emailDF$spamIndic[2]) %>%
  kable_styling(bootstrap_options = c("striped","condensed","responsive"),full_width   = F,position = "left",font_size = 12) %>%
  row_spec(0, background ="gray")

#shuffle the data rowwise
emails <- emailDF[sample(nrow(emailDF)),]

#Performing some basic analytics functions on the randomised dataframe.

#calculate the maximum number of characters in an email
max(nchar(emails$text))
#print the minimum number of characters in an email
min(nchar(emails$text))
#removing  rows with text columns as blank
emails <- emails[-which(emails$text==""),]
#again runnjinjg the function to calculate minmum characters in an email
min(nchar(emails$text))
#print the row which has minmum number of characters in an email
which.min(nchar(emails$text))

kable(head(table(emails$spamIndic))) %>%
  kable_styling(bootstrap_options = c("striped","condensed","responsive"),full_width   = F,position = "left",font_size = 12) %>%
  row_spec(0, background ="gray")

```
##Step3 :-
Creating a corpus             
Follow the standard steps to build and pre-process the corpus:                       
**1) Build a new corpus variable called corpus.**                      
**2) Using tm_map, convert the text to lowercase.**                     
**3) Using tm_map, remove all punctuation from the corpus.**                     
**4) Using tm_map, remove all English stopwords from the corpus.**                 
**5) Using tm_map, stem the words in the corpus.**                        
**6) Build a document term matrix from the corpus, called dtm.**                    


```{r warning=FALSE, echo=FALSE}
corpus <- Corpus(VectorSource(emails$text))
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, unique)
```
                        
##Step4:-         
Further pre-processing of the corpus,Removing stop words.emoving words can be done with the removeWords argument to the tm_map() function, with an extra argument, i.e. what the stop words are that we want to remove, for which we simply use the list for english that is provided by the tm package.                     

```{r warning=FALSE , echo=FALSE}

corpus <- tm_map(corpus, removeWords, stopwords("english"))
```
                        
##Step5:-           
Stemming :-             
Lastly, we want to stem our document with the stemDocument argument.Stemming is nothing but collapsing/removing words which have same root.                        

```{r}

corpus <- tm_map(corpus, stemDocument)

```
                    
##Step6:-         
BAG OF WORDS             
Create a Document Term Matrix                     
We are now ready to extract the word frequencies to be used in our prediction problem. The tm package provides a function called DocumentTermMatrix() that generates a matrix where:                   
--the rows correspond to documents,and                                          
--the columns correspond to words .                      
--The values in the matrix are the number of times that word appears in each document.                      

```{r}
tdm = DocumentTermMatrix(corpus)
#kable(head(tdm)) %>%
#  kable_styling(bootstrap_options = c("striped","condensed","responsive"),full_width   = F,position = "left",font_size = 12) %>%
#  row_spec(0, background ="gray")

```
##Step7:-
Using sparseRemove to remove the terms which are not often appear or they don't create any additional benefits but removing them adds to computation benefits,to obtain a more reasonable number of terms, limit dtm to contain terms appearing in at least 5% of documents.             
                            
Remove sparse terms.                      
```{r warning=FALSE, echo=FALSE}
spdtm = removeSparseTerms(tdm, 0.95)
#kable(head(spdtm)) %>%
#  kable_styling(bootstrap_options = c("striped","condensed","responsive"),full_width   = F,position = "left",font_size = 12) %>%
#  row_spec(0, background ="gray")

#build a data frame from sparse term frequency document matrix
emailsSparse = as.data.frame(as.matrix(spdtm))

#use the make.names function to make syntactically valid names out of character vectors(emailsSparse).
colnames(emailsSparse) = make.names(colnames(emailsSparse))

kable(head(sort(colSums(emailsSparse)))) %>%
  kable_styling(bootstrap_options = c("striped","condensed","responsive"),full_width   = F,position = "left",font_size = 12) %>%
  row_spec(0, background ="gray")

#Add a variable called "spam" to emailsSparse containing the email spam labels, this can be done by copying over the "spam" variable from the original data frame.
emailsSparse$spam = emails$spamIndic
emailsSparse$spam = emails$spam

```

Now let's see how many time word stems appear in the ham emails in the dataset We can read the most frequent terms in the ham dataset and the spam subsets.
```{r}
tail(sort(colSums(subset(emailsSparse, spam == FALSE))))
#kable(tail(sort(colSums(subset(emailsSparse, spam == FALSE)))) %>%
#  kable_styling(bootstrap_options = c("striped","condensed","responsive"),full_width   = F,position = "left",font_size = 12) %>%
#  row_spec(0, background ="gray")


head(sort(colSums(subset(emailsSparse, spam == TRUE))))
#kable(head(sort(colSums(subset(emailsSparse, spam == TRUE))))) %>%
#  kable_styling(bootstrap_options = c("striped","condensed","responsive"),full_width   = F,position = "left",font_size = 12) %>%
#  row_spec(0, background ="gray")
```
                
##Step8 :-                         
Build Machine learnig model (SVM i.e. Support Vector Machine)                         
First, convert the dependent variable to a factor .  
Before building the model we need to split our data into training and testing by using sample.split function with 70 data in training and rest in test.             
Train and Test Subset, use the subset function TRUE for the train and FALSE for the test.                    
After training the data, testing the data against the test data to validate out model and ascertain the accuracy of our Model(SVM).               
                     
```{r warning=FALSE, echo=FALSE}
emailsSparse$spam = as.factor(emailsSparse$spam)
#we are setting the random seed some value so that every time same result will come.
set.seed(123)
spl = sample.split(emailsSparse$spam, 0.7)

index <- 1:nrow(emailsSparse)

dim(emailsSparse)
trainIndex <- sample(index, trunc(length(index) * 0.666666666666667))

emailsSparse.train <- emailsSparse[trainIndex, ]

print(paste0("Percentage: ", round((nrow(emailsSparse.train)/nrow(emailsSparse)) * 100, 
    2), " %"))

emailsSparse.test <- emailsSparse[-trainIndex, ]
print(paste0("Percentage: ", round((nrow(emailsSparse.test)/nrow(emailsSparse)) * 100, 
    2), " %"))

model.svm <- svm(emailsSparse.train$spam ~ ., method = "class", emailsSparse.train)

summary(model.svm)

prediction.svm <- predict(model.svm, newdata = emailsSparse.test, type = "class")
table(`Actual Class` = emailsSparse.test$spam, `Predicted Class` = prediction.svm)

#Accuracy
(20+208)/nrow(emailsSparse.test)
```
##Step9:-             
BUILD A CART MODEL (Decision Tree)                    
Plotting the decission tree at the end.            
```{r}
spamCART = rpart(emailsSparse.train$spam~., data=emailsSparse.train, method="class")
prp(spamCART)
predTestCART = predict(spamCART, newdata=emailsSparse.test)[,2]

#What is the testing set AUC of spamCART?
#predictionTestCART = prediction(predTestCART, emailsSparse.test$spam)
#as.numeric(performance(predictionTestCART, "auc")@y.values)

#What is the testing set accuracy of spamRF, using a threshold of 0.5 for predictions?

table(emailsSparse.test$spam, predTestCART  > 0.5)

#Accuracy
(20+208)/nrow(emailsSparse.test)
```
                    
##Step10:-                                          
BUILD A RANDOM FOREST                
We are creating one more model random forest model and called this model as spamRF.
Create Random Forest Model using all the parameters.               

```{r}


spamRF = randomForest(emailsSparse.train$spam~., data=emailsSparse.train)
predTrainRF = predict(spamRF, type="prob")[,2]

#What is the training set accuracy of spamRF, using a threshold of 0.5 for predictions?

#table(emailsSparse.train$spam, predTrainRF > 0.5)
#Accuracy
#(76+412)/nrow(emailsSparse.test)
```

Out-of-Sample Performance of the above model(Random Forest) we created
Now that we have trained a models, we need to evaluate it on the test set, and also calc ulate the accuracy of each model.

```{r}
#Accuracy of RAndom Forest model against test data
predTestRF = predict(spamRF, newdata=emailsSparse.test, type="prob")[,2]
table(emailsSparse.test$spam, predTestRF > 0.5)
#Accuracy
(31+207)/nrow(emailsSparse.test)
#predictionTestRF = prediction(predTestRF, emailsSparse.test$spam)
#as.numeric(performance(predictionTestRF, "auc")@y.values)
```

#Summary
Looking at above models implemented for our test dataset, and the accuracy predicted for all the three models it seems that Random Forest Model's accuracy(93%) is more accurate than CART Model(Decision Model  89%) & SVM (89% Support Vector Machine ) Model.